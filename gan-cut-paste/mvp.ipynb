{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b1e5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 51\n",
      "Number of matched pairs: 51\n",
      "Number of backgrounds: 1\n",
      "Successfully loaded first sample\n",
      "Sample keys: dict_keys(['naive_composite', 'ground_truth', 'border_mask', 'cut_object', 'new_background'])\n",
      "naive_composite shape: torch.Size([3, 256, 256])\n",
      "ground_truth shape: torch.Size([3, 256, 256])\n",
      "border_mask shape: torch.Size([1, 256, 256])\n",
      "cut_object shape: torch.Size([3, 256, 256])\n",
      "new_background shape: torch.Size([3, 256, 256])\n",
      "Successfully loaded first batch\n",
      "Batch size: 4\n",
      "Epoch [1/1], Step [1/13], D_loss: 0.6988, G_loss: 5.5829\n",
      "Epoch [1/1], Avg D_loss: 0.5976, Avg G_loss: 3.9409\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class BorderDataset(Dataset):\n",
    "    def __init__(self, cut_objects_path, original_images_path, new_backgrounds_path, transform=None, image_size=256):\n",
    "        self.cut_objects_path = Path(cut_objects_path)\n",
    "        self.original_images_path = Path(original_images_path)\n",
    "        self.new_backgrounds_path = Path(new_backgrounds_path)\n",
    "        self.transform = transform\n",
    "        self.image_size = image_size\n",
    "\n",
    "        # Get all image files\n",
    "        self.cut_objects = list(self.cut_objects_path.glob(\n",
    "            '*.jpg')) + list(self.cut_objects_path.glob('*.png'))\n",
    "        self.original_images = list(self.original_images_path.glob(\n",
    "            '*.jpg')) + list(self.original_images_path.glob('*.png'))\n",
    "        self.new_backgrounds = list(self.new_backgrounds_path.glob(\n",
    "            '*.jpg')) + list(self.new_backgrounds_path.glob('*.png'))\n",
    "\n",
    "        # Match by filename\n",
    "        self.matched_pairs = []\n",
    "        for cut_obj in self.cut_objects:\n",
    "            # Find corresponding original image\n",
    "            original_match = None\n",
    "            for orig in self.original_images:\n",
    "                if cut_obj.stem == orig.stem:\n",
    "                    original_match = orig\n",
    "                    break\n",
    "\n",
    "            if original_match:\n",
    "                self.matched_pairs.append((cut_obj, original_match))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.matched_pairs) * len(self.new_backgrounds)\n",
    "\n",
    "    def create_border_mask(self, cut_object, border_width=10):\n",
    "        \"\"\"Create a mask for the border region\"\"\"\n",
    "        try:\n",
    "            # Convert to grayscale and create alpha mask\n",
    "            if len(cut_object.shape) == 3 and cut_object.shape[2] == 4:  # RGBA\n",
    "                alpha = cut_object[:, :, 3].astype(np.float32) / 255.0\n",
    "            else:  # RGB - create alpha from non-black pixels\n",
    "                alpha = np.any(cut_object > 10, axis=2).astype(np.float32)\n",
    "\n",
    "            # Ensure alpha is in correct range and type\n",
    "            alpha = np.clip(alpha, 0, 1).astype(np.uint8)\n",
    "\n",
    "            # Create border mask using morphological operations\n",
    "            kernel = np.ones((border_width, border_width), np.uint8)\n",
    "            dilated = cv2.dilate(alpha, kernel, iterations=1)\n",
    "            eroded = cv2.erode(alpha, kernel, iterations=1)\n",
    "            border_mask = dilated - eroded\n",
    "\n",
    "            return border_mask.astype(np.float32)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating border mask: {e}\")\n",
    "            print(f\"Cut object shape: {cut_object.shape}\")\n",
    "            # Return a default mask in case of error\n",
    "            return np.ones((cut_object.shape[0], cut_object.shape[1]), dtype=np.float32) * 0.1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            # Get pair index and background index\n",
    "            pair_idx = idx // len(self.new_backgrounds)\n",
    "            bg_idx = idx % len(self.new_backgrounds)\n",
    "\n",
    "            cut_obj_path, original_path = self.matched_pairs[pair_idx]\n",
    "            bg_path = self.new_backgrounds[bg_idx]\n",
    "\n",
    "            # Load images with error handling\n",
    "            cut_object = cv2.imread(str(cut_obj_path), cv2.IMREAD_UNCHANGED)\n",
    "            original = cv2.imread(str(original_path))\n",
    "            new_background = cv2.imread(str(bg_path))\n",
    "\n",
    "            # Check if images loaded successfully\n",
    "            if cut_object is None:\n",
    "                raise ValueError(f\"Failed to load cut object: {cut_obj_path}\")\n",
    "            if original is None:\n",
    "                raise ValueError(\n",
    "                    f\"Failed to load original image: {original_path}\")\n",
    "            if new_background is None:\n",
    "                raise ValueError(f\"Failed to load background: {bg_path}\")\n",
    "\n",
    "            # Convert BGR to RGB with proper error handling\n",
    "            if len(cut_object.shape) == 3:\n",
    "                if cut_object.shape[2] == 3:\n",
    "                    cut_object = cv2.cvtColor(cut_object, cv2.COLOR_BGR2RGB)\n",
    "                elif cut_object.shape[2] == 4:\n",
    "                    cut_object = cv2.cvtColor(cut_object, cv2.COLOR_BGRA2RGBA)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Unexpected cut object shape: {cut_object.shape}\")\n",
    "\n",
    "            if len(original.shape) == 3 and original.shape[2] == 3:\n",
    "                original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Unexpected original image shape: {original.shape}\")\n",
    "\n",
    "            if len(new_background.shape) == 3 and new_background.shape[2] == 3:\n",
    "                new_background = cv2.cvtColor(\n",
    "                    new_background, cv2.COLOR_BGR2RGB)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Unexpected background shape: {new_background.shape}\")\n",
    "\n",
    "            # Resize images\n",
    "            cut_object = cv2.resize(\n",
    "                cut_object, (self.image_size, self.image_size))\n",
    "            original = cv2.resize(original, (self.image_size, self.image_size))\n",
    "            new_background = cv2.resize(\n",
    "                new_background, (self.image_size, self.image_size))\n",
    "\n",
    "            # Create border mask\n",
    "            border_mask = self.create_border_mask(cut_object)\n",
    "\n",
    "            # Create naive composite (input to generator)\n",
    "            if len(cut_object.shape) == 3 and cut_object.shape[2] == 4:  # RGBA\n",
    "                alpha = cut_object[:, :, 3:4] / 255.0\n",
    "                cut_object_rgb = cut_object[:, :, :3]\n",
    "            else:  # RGB\n",
    "                alpha = np.any(cut_object > 10, axis=2,\n",
    "                               keepdims=True).astype(np.float32)\n",
    "                cut_object_rgb = cut_object\n",
    "\n",
    "            # Ensure all arrays are float32 for consistency\n",
    "            alpha = alpha.astype(np.float32)\n",
    "            cut_object_rgb = cut_object_rgb.astype(np.float32)\n",
    "            new_background = new_background.astype(np.float32)\n",
    "\n",
    "            naive_composite = cut_object_rgb * \\\n",
    "                alpha + new_background * (1 - alpha)\n",
    "\n",
    "            # Ground truth is the original image\n",
    "            ground_truth = original.astype(np.float32)\n",
    "\n",
    "            # Apply transforms\n",
    "            if self.transform:\n",
    "                # Ensure images are in uint8 range for PIL transforms\n",
    "                naive_composite = np.clip(\n",
    "                    naive_composite, 0, 255).astype(np.uint8)\n",
    "                ground_truth = np.clip(ground_truth, 0, 255).astype(np.uint8)\n",
    "                cut_object_rgb = np.clip(\n",
    "                    cut_object_rgb, 0, 255).astype(np.uint8)\n",
    "                new_background = np.clip(\n",
    "                    new_background, 0, 255).astype(np.uint8)\n",
    "\n",
    "                naive_composite = self.transform(naive_composite)\n",
    "                ground_truth = self.transform(ground_truth)\n",
    "                border_mask = torch.tensor(\n",
    "                    border_mask, dtype=torch.float32).unsqueeze(0)\n",
    "                cut_object_rgb = self.transform(cut_object_rgb)\n",
    "                new_background = self.transform(new_background)\n",
    "            else:\n",
    "                # Convert to tensors if no transform\n",
    "                naive_composite = torch.tensor(naive_composite.transpose(\n",
    "                    2, 0, 1), dtype=torch.float32) / 255.0\n",
    "                ground_truth = torch.tensor(ground_truth.transpose(\n",
    "                    2, 0, 1), dtype=torch.float32) / 255.0\n",
    "                border_mask = torch.tensor(\n",
    "                    border_mask, dtype=torch.float32).unsqueeze(0)\n",
    "                cut_object_rgb = torch.tensor(cut_object_rgb.transpose(\n",
    "                    2, 0, 1), dtype=torch.float32) / 255.0\n",
    "                new_background = torch.tensor(new_background.transpose(\n",
    "                    2, 0, 1), dtype=torch.float32) / 255.0\n",
    "\n",
    "            return {\n",
    "                'naive_composite': naive_composite,\n",
    "                'ground_truth': ground_truth,\n",
    "                'border_mask': border_mask,\n",
    "                'cut_object': cut_object_rgb,\n",
    "                'new_background': new_background\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing item {idx}: {e}\")\n",
    "            print(f\"Cut object path: {cut_obj_path}\")\n",
    "            print(f\"Original path: {original_path}\")\n",
    "            print(f\"Background path: {bg_path}\")\n",
    "            raise e\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_channels=3, output_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Input: 3 x 256 x 256\n",
    "            nn.Conv2d(input_channels, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 64 x 128 x 128\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 128 x 64 x 64\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 256 x 32 x 32\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 512 x 16 x 16\n",
    "            nn.Conv2d(512, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            # 512 x 8 x 8\n",
    "            nn.ConvTranspose2d(512, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 512 x 16 x 16\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 256 x 32 x 32\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 128 x 64 x 64\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 64 x 128 x 128\n",
    "            nn.ConvTranspose2d(64, output_channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # 3 x 256 x 256\n",
    "        )\n",
    "\n",
    "        # Skip connections layers\n",
    "        self.skip_conv1 = nn.Conv2d(64, 64, 1)\n",
    "        self.skip_conv2 = nn.Conv2d(128, 128, 1)\n",
    "        self.skip_conv3 = nn.Conv2d(256, 256, 1)\n",
    "        self.skip_conv4 = nn.Conv2d(512, 512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode with skip connections\n",
    "        enc1 = self.encoder[:2](x)  # 64 x 128 x 128\n",
    "        enc2 = self.encoder[2:5](enc1)  # 128 x 64 x 64\n",
    "        enc3 = self.encoder[5:8](enc2)  # 256 x 32 x 32\n",
    "        enc4 = self.encoder[8:11](enc3)  # 512 x 16 x 16\n",
    "        enc5 = self.encoder[11:](enc4)  # 512 x 8 x 8\n",
    "\n",
    "        # Decode with skip connections\n",
    "        dec1 = self.decoder[:3](enc5)  # 512 x 16 x 16\n",
    "        dec1 = dec1 + self.skip_conv4(enc4)\n",
    "\n",
    "        dec2 = self.decoder[3:6](dec1)  # 256 x 32 x 32\n",
    "        dec2 = dec2 + self.skip_conv3(enc3)\n",
    "\n",
    "        dec3 = self.decoder[6:9](dec2)  # 128 x 64 x 64\n",
    "        dec3 = dec3 + self.skip_conv2(enc2)\n",
    "\n",
    "        dec4 = self.decoder[9:12](dec3)  # 64 x 128 x 128\n",
    "        dec4 = dec4 + self.skip_conv1(enc1)\n",
    "\n",
    "        output = self.decoder[12:](dec4)  # 3 x 256 x 256\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            # Input: 3 x 256 x 256\n",
    "            nn.Conv2d(input_channels, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 64 x 128 x 128\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 128 x 64 x 64\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 256 x 32 x 32\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 512 x 16 x 16\n",
    "            nn.Conv2d(512, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 512 x 8 x 8\n",
    "        )\n",
    "\n",
    "        # Global average pooling to reduce spatial dimensions\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Final classification layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        pooled = self.global_pool(features)\n",
    "        flattened = pooled.view(pooled.size(0), -1)  # [batch_size, 512]\n",
    "        output = self.classifier(flattened)\n",
    "        return output.squeeze(1)  # [batch_size]\n",
    "\n",
    "\n",
    "class BorderGAN:\n",
    "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize networks\n",
    "        self.generator = Generator().to(device)\n",
    "        self.discriminator = Discriminator().to(device)\n",
    "\n",
    "        # Loss functions\n",
    "        self.adversarial_loss = nn.BCELoss()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "        # Optimizers\n",
    "        self.g_optimizer = optim.Adam(\n",
    "            self.generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "        self.d_optimizer = optim.Adam(\n",
    "            self.discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "        # Initialize weights\n",
    "        self.generator.apply(self.weights_init)\n",
    "        self.discriminator.apply(self.weights_init)\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Conv') != -1:\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        elif classname.find('BatchNorm') != -1:\n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        naive_composite = batch['naive_composite'].to(self.device)\n",
    "        ground_truth = batch['ground_truth'].to(self.device)\n",
    "        border_mask = batch['border_mask'].to(self.device)\n",
    "\n",
    "        batch_size = naive_composite.size(0)\n",
    "\n",
    "        # Labels for adversarial loss\n",
    "        real_label = torch.ones(batch_size, device=self.device)\n",
    "        fake_label = torch.zeros(batch_size, device=self.device)\n",
    "\n",
    "        # Train Discriminator\n",
    "        self.d_optimizer.zero_grad()\n",
    "\n",
    "        # Real images\n",
    "        real_output = self.discriminator(ground_truth)\n",
    "        d_real_loss = self.adversarial_loss(real_output, real_label)\n",
    "\n",
    "        # Fake images\n",
    "        fake_images = self.generator(naive_composite)\n",
    "        fake_output = self.discriminator(fake_images.detach())\n",
    "        d_fake_loss = self.adversarial_loss(fake_output, fake_label)\n",
    "\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        self.d_optimizer.step()\n",
    "\n",
    "        # Train Generator\n",
    "        self.g_optimizer.zero_grad()\n",
    "\n",
    "        # Adversarial loss\n",
    "        fake_output = self.discriminator(fake_images)\n",
    "        g_adv_loss = self.adversarial_loss(fake_output, real_label)\n",
    "\n",
    "        # L1 loss (focus on border regions)\n",
    "        l1_loss = self.l1_loss(fake_images * border_mask,\n",
    "                               ground_truth * border_mask)\n",
    "\n",
    "        # Perceptual loss (MSE on the entire image)\n",
    "        perceptual_loss = self.mse_loss(fake_images, ground_truth)\n",
    "\n",
    "        # Combined generator loss\n",
    "        g_loss = g_adv_loss + 100 * l1_loss + 10 * perceptual_loss\n",
    "        g_loss.backward()\n",
    "        self.g_optimizer.step()\n",
    "\n",
    "        return {\n",
    "            'd_loss': d_loss.item(),\n",
    "            'g_loss': g_loss.item(),\n",
    "            'g_adv_loss': g_adv_loss.item(),\n",
    "            'l1_loss': l1_loss.item(),\n",
    "            'perceptual_loss': perceptual_loss.item()\n",
    "        }\n",
    "\n",
    "    def train(self, dataloader, num_epochs=100, save_interval=10):\n",
    "        self.generator.train()\n",
    "        self.discriminator.train()\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_d_loss = 0\n",
    "            epoch_g_loss = 0\n",
    "\n",
    "            for i, batch in enumerate(dataloader):\n",
    "                losses = self.train_step(batch)\n",
    "                epoch_d_loss += losses['d_loss']\n",
    "                epoch_g_loss += losses['g_loss']\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], '\n",
    "                          f'D_loss: {losses[\"d_loss\"]:.4f}, G_loss: {losses[\"g_loss\"]:.4f}')\n",
    "\n",
    "            avg_d_loss = epoch_d_loss / len(dataloader)\n",
    "            avg_g_loss = epoch_g_loss / len(dataloader)\n",
    "\n",
    "            print(\n",
    "                f'Epoch [{epoch+1}/{num_epochs}], Avg D_loss: {avg_d_loss:.4f}, Avg G_loss: {avg_g_loss:.4f}')\n",
    "\n",
    "            if (epoch + 1) % save_interval == 0:\n",
    "                self.save_models(f'border_gan_epoch_{epoch+1}.pth')\n",
    "                self.generate_samples(dataloader, epoch+1)\n",
    "\n",
    "    def save_models(self, path):\n",
    "        torch.save({\n",
    "            'generator_state_dict': self.generator.state_dict(),\n",
    "            'discriminator_state_dict': self.discriminator.state_dict(),\n",
    "            'g_optimizer_state_dict': self.g_optimizer.state_dict(),\n",
    "            'd_optimizer_state_dict': self.d_optimizer.state_dict(),\n",
    "        }, path)\n",
    "\n",
    "    def load_models(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "        self.discriminator.load_state_dict(\n",
    "            checkpoint['discriminator_state_dict'])\n",
    "        self.g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n",
    "        self.d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])\n",
    "\n",
    "    def generate_samples(self, dataloader, epoch):\n",
    "        self.generator.eval()\n",
    "        with torch.no_grad():\n",
    "            batch = next(iter(dataloader))\n",
    "            naive_composite = batch['naive_composite'][:4].to(self.device)\n",
    "            ground_truth = batch['ground_truth'][:4].to(self.device)\n",
    "\n",
    "            fake_images = self.generator(naive_composite)\n",
    "\n",
    "            # Save comparison images\n",
    "            comparison = torch.cat(\n",
    "                [naive_composite, fake_images, ground_truth], dim=3)\n",
    "            save_image(\n",
    "                comparison, f'samples_epoch_{epoch}.png', nrow=1, normalize=True)\n",
    "\n",
    "        self.generator.train()\n",
    "\n",
    "    def blend_image(self, cut_object_path, new_background_path, output_path):\n",
    "        \"\"\"Blend a cut object with a new background using the trained generator\"\"\"\n",
    "        self.generator.eval()\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "\n",
    "        # Load and preprocess images\n",
    "        cut_object = cv2.imread(cut_object_path, cv2.IMREAD_UNCHANGED)\n",
    "        new_background = cv2.imread(new_background_path)\n",
    "\n",
    "        cut_object = cv2.cvtColor(cut_object, cv2.COLOR_BGR2RGB) if cut_object.shape[2] == 3 else cv2.cvtColor(\n",
    "            cut_object, cv2.COLOR_BGRA2RGBA)\n",
    "        new_background = cv2.cvtColor(new_background, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Resize\n",
    "        cut_object = cv2.resize(cut_object, (256, 256))\n",
    "        new_background = cv2.resize(new_background, (256, 256))\n",
    "\n",
    "        # Create naive composite\n",
    "        if cut_object.shape[2] == 4:\n",
    "            alpha = cut_object[:, :, 3:4] / 255.0\n",
    "            cut_object_rgb = cut_object[:, :, :3]\n",
    "        else:\n",
    "            alpha = np.any(cut_object > 10, axis=2,\n",
    "                           keepdims=True).astype(np.float32)\n",
    "            cut_object_rgb = cut_object\n",
    "\n",
    "        naive_composite = cut_object_rgb * alpha + new_background * (1 - alpha)\n",
    "\n",
    "        # Transform and generate\n",
    "        naive_composite_tensor = transform(\n",
    "            naive_composite.astype(np.uint8)).unsqueeze(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            blended_image = self.generator(naive_composite_tensor)\n",
    "\n",
    "        # Save result\n",
    "        save_image(blended_image, output_path, normalize=True)\n",
    "        print(f\"Blended image saved to {output_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    cut_objects_path = \"cut_objects\"\n",
    "    original_images_path = \"original_images\"\n",
    "    new_backgrounds_path = \"new_backgrounds\"\n",
    "\n",
    "    # Data transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = BorderDataset(\n",
    "        cut_objects_path, original_images_path, new_backgrounds_path, transform)\n",
    "\n",
    "    # Check if dataset is valid\n",
    "    if len(dataset) == 0:\n",
    "        print(\"Error: Dataset is empty. Check your file paths and ensure images exist.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Dataset size: {len(dataset)}\")\n",
    "    print(f\"Number of matched pairs: {len(dataset.matched_pairs)}\")\n",
    "    print(f\"Number of backgrounds: {len(dataset.new_backgrounds)}\")\n",
    "\n",
    "    # Test loading a single item first\n",
    "    try:\n",
    "        sample = dataset[0]\n",
    "        print(\"Successfully loaded first sample\")\n",
    "        print(f\"Sample keys: {sample.keys()}\")\n",
    "        for key, value in sample.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                print(f\"{key} shape: {value.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading first sample: {e}\")\n",
    "        return\n",
    "\n",
    "    # Use num_workers=0 to avoid multiprocessing issues during debugging\n",
    "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "\n",
    "    # Test the dataloader\n",
    "    try:\n",
    "        batch = next(iter(dataloader))\n",
    "        print(\"Successfully loaded first batch\")\n",
    "        print(f\"Batch size: {batch['naive_composite'].shape[0]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading batch: {e}\")\n",
    "        return\n",
    "\n",
    "    # Initialize and train model\n",
    "    border_gan = BorderGAN()\n",
    "\n",
    "    # Train the model\n",
    "    border_gan.train(dataloader, num_epochs=1, save_interval=1)\n",
    "\n",
    "    # Example usage for blending\n",
    "    # border_gan.load_models('border_gan_epoch_100.pth')\n",
    "    # border_gan.blend_image('cut_objects/example.png', 'new_backgrounds/bg1.jpg', 'result.png')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c223da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blended image saved to result.png\n"
     ]
    }
   ],
   "source": [
    "border_gan = BorderGAN()\n",
    "border_gan.load_models('border_gan_epoch_1.pth')\n",
    "border_gan.blend_image('cut_objects/Tardigrade_01_0002.png',\n",
    "                       'new_backgrounds/Bandwurm_01_0001.png', 'result.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan-cut-paste",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
